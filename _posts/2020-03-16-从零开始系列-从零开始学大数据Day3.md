---
layout:     post
title:      从零开始学大数据
subtitle:   Day3
date:       2020-03-16
author:     KaithyXu
header-img: img/elephant2.jpg
catalog: true
tags:
    - Big Data
---

## 记录
自己有好多不懂的技术，好在自己还年轻，什么都可以尝试，所以打算通过写博客的方式倒逼自己学习，博客会记录自己的学习经历，包括付费课程，博客，书籍等等。
这是从零开始系列的第一天，往后的日子还有很长，希望自己能够一直坚持下去，不要忘记自己的初心，努力成为一个技术大牛！

## Hive & Spark -- 极客时间系列

最近在通过极客时间学习大数据，所以就先以这个作为起点。
### Hive
Hive只能处理离线数据
核心概念：数据仓库，解释器，编译器，优化器；运行时元数据存储在关系型数据库中

对于DDL语句，Hive会通过执行引擎Driver将数据表的信息记录在MetaStore元数据组件中，这个组件通常用一个关系型数据库实现，记录表名，字段名，字段类型，关联HDFS文件路径等数据库的Meta信息

对于DQL语句，Driver会将该语句提交给自己的编译器Compiler进行语法分析，解析，优化等操作，最后生成一个MR执行计划，根据计划生成一个MR的作业，提交给Hadoop MR计算框架处理

Hive处理Join操作：
由于join设计两张表，来自两个文件，需要在map输出的时候进行标记，如第一张表输出的Value记录为<1,X>，这里的1表示数据来自第一张表。
经过shuffle后，相同的key被输入到同一个reduce函数中，就可以根据表的标记对value数据求笛卡尔积

Hive架构图：
![](https://static001.geekbang.org/resource/image/26/ea/26287cac9a9cfa3874a680fdbcd795ea.jpg)

### Spark
RDD是Spark的核心概念，是弹性数据集的缩写。RDD既是Spark面向开发者的编程模型，也是Spark自身架构的核心元素。

Spark是直接对数据进行编程，将大规模数据集合抽象成一个RDD对象，然后在这个RDD上进行各种计算，得到一个新的RDD，继续计算处理，知道得到最后的结果数据

Spark可以理解为面向对象的大数据计算
RDD定义的函数分为两种：
* 转换函数
    * 转换操作产生的RDD不会出现新的分片，结果还是当前分片
    * 转换操作产生的RDD会出现新的分片
* 执行函数

Spark也是对大数据进行分片计算，Spark分布式计算的数据分片，任务调度都是以RDD为单位展开的，每一个RDD分片都会分配到一个执行进程去处理

Spark应用程序中的RDD和Spark执行过程中生成的物理RDD不是一一对应的

### Spark 的计算阶段

![](https://static001.geekbang.org/resource/image/c8/db/c8cf515c664b478e51058565e0d4a8db.png)

Spark可以根据应用的复杂程度，分割成更多的计算阶段(stage)，这些计算阶段组成一个有向无环图DAG，Spark任务调度器可以根据DAG的依赖关系执行计算阶段

所谓 DAG 也就是有向无环图，就是说不同阶段的依赖关系是有向的，计算过程只能沿着依赖关系方向执行，被依赖的阶段执行完成之前，依赖的阶段不能开始执行，同时，这个依赖关系不能有环形依赖，否则就成为死循环了。

Spark作业调度执行的核心是DAG
负责Spark应用DAG生成和管理的组件是DAGScheduler，DAGScheduler根据程序代码生成DAG，然后将程序分发到分布式计算集群，按计算阶段的先后关系调度执行

当RDD之间的转换连线呈现多对多交叉连接的时候，就会产生新的阶段。一个RDD代表一个数据集，每个RDD中都包含了多个小块，每个小块代表RDD的一个分片

Spark也需要通过shuffle将数据进行重新组合，相同Key的数据放到一起，进行聚合，关联等操作，因而每次shuffle对产生新的计算阶段

计算阶段划分的依据是shuffle，而不是转换函数的类型

Spark优先使用内存进行数据存储，包括RDD数据

### Spark的作业管理
Spark的DAGScheduler在遇到shuffle时，会生成一个计算阶段，在遇到action函数时，会生成一个作业

RDD中的每个数据分片，Spark都会创建一个计算任务去处理，所以一个计算阶段会包含多个计算任务

### Spark的执行过程
![](https://static001.geekbang.org/resource/image/16/db/164e9460133d7744d0315a876e7b6fdb.png)

1. Spark应用程序启动在自己的JVM进程中，即Driver进程，启动会调用SparkContext初始化执行配置和输入数据。
2. SparkContext启动DAGScheduler构造执行的DAG图，切分成最小的执行单位，即计算任务
3. Driver向Cluster Manager请求计算资源，用于DAG的分布式计算，Cluster Manager在收到消息后会将Drivere的信息发送给所有计算节点Worker
4. Worker收到消息后会向Driver进行通信并注册，根据空闲资源向Driver通报可以处理的任务数，Driver根据DAG图向Worker分配任务
5. Worker收到任务后，启动Executor进行开始执行任务

Spark三个主要的特性：RDD的编程模型，DAG泄愤的多阶段计算过，使用内存存储中间计算结果
