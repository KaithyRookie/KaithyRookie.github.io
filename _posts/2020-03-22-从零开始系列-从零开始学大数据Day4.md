---
layout:     post
title:      从零开始学大数据
subtitle:   Day4
date:       2020-03-22
author:     KaithyXu
header-img: img/tiger.jpg
catalog: true
tags:
    - Big Data
---

## 记录
自己有好多不懂的技术，好在自己还年轻，什么都可以尝试，所以打算通过写博客的方式倒逼自己学习，博客会记录自己的学习经历，包括付费课程，博客，书籍等等。
这是从零开始系列的第一天，往后的日子还有很长，希望自己能够一直坚持下去，不要忘记自己的初心，努力成为一个技术大牛！

## MapReduce -- 《Hadoop权威指南》

### NCDC数据准备
以下步骤皆默认Hadoop已经启动。

#### 数据下载
数据准备阶段结合Hadoop权威指南附录以及参考[这篇博客](http://isunix.github.io/blog/2019/05/30/hadoopquan-wei-zhi-nan-ncdcshu-ju-zhun-bei-gong-zuo-bei-wang/)。

《Hadoop权威指南》一书中使用的NCDC气象数据，由于其GitHub仓库中没有了，所以周末我花了一些时间去NCDC官网下了一部分数据(2000~2019年)
因为只是为了学习Hadoop，所所以没有吧所有数据下载下来，数据我保存在了[GitHub仓库](https://github.com/KaithyRookie/ncdc_data_set) ，有需要的可以自取。
如果想要下载其他年份的数据，可以参考如下的下载脚本:

```
#!/bin/bash

#这里cd到你想下载到的目录, 每个文件的格式如下所示gsod_1901.tar
cdir=/root/ncdc_data

for i in $(seq 2010 2011)
do
	name=gsod_$i.tar
    wget --execute robots=off —accept=tar -r -np -nH --cut-dirs=4 - R index.html* ftp://ftp.ncdc.noaa.gov/pub/data/gsod/$i/$name
done

```

下载下来的数据包是以tar为后缀的一系列压缩包

#### 数据文件上传
1.创建压缩文件的保存目录以及最终整合成大文件后的保存目录

```
hdfs dfs -mkdir /NCDC /NCDC_ALL
```
2.将下载的所有压缩文件上传到NCDC目录下
这里我将所有下载的压缩文件放到了/ncdc/这个目录下

```
hdfs dfs -put /ncdc/* /NCDC?
```
3.生成MR的Input文件

生成的脚本如下：

```
#!/bin/bash

a=$1
rm -rf ncdc_files.txt
hdfs dfs -rm /ncdc_files.txt

while [ $a -le $2 ]; do
	#statements
	filename="/NCDC/gsod_${a}.tar"
	echo "$filename" >> ncdc_files.txt
	a=`expr $a + 1`
done

hdfs dfs -put ncdc_files.txt /
```
执行命令 
```
sh prepare.sh 2000 2019 
```
prepare.sh是该脚本的文件名。
ncdc_files.txt文件中的每一行都是/NCDC/gsod_2000.tar这样的格式

4.根据附录，由于Hadoop对少量大文件的处理更容易，所以需要将每一年的所有数据整合到一个文件中
脚本如下：

```
#!/bin/bash

read offset hdfs_file
echo -e "$offset\t$hdfs_file"

# Retrieve file from HDFS to local disk
echo "reporter:status:Retrieving $hdfs_file" >&2
$HADOOP_HOME/bin/hdfs dfs -get $hdfs_file .
# Create local directory
target=`basename $hdfs_file .tar`
mkdir $target

echo "reporter:status:Un-tarring $hdfs_file to $target" >&2
tar xf `basename $hdfs_file` -C $target
# Unzip each station file and concat into one file
echo "reporter:status:Un-gzipping $target" >&2
for file in $target/*
do
        gunzip -c $file >> $target.all
        echo "reporter:status:Processed $file" >&2
done
# Put gzipped version into HDFS
echo "reporter:status:Gzipping $target and putting in HDFS" >&2
gzip -c $target.all | $HADOOP_HOME/bin/hdfs  dfs -put - /NCDC_ALL/$target.gz
rm `basename $hdfs_file`
rm -r $target
rm $target.all
```

命令行执行命令：

```
hadoop jar ${HADOOP_HOME}/share/hadoop/tools/lib/hadoop-streaming-3.2.1.jar \
    -D mapreduce.job.reduces=0 \
    -D mapreduce.map.speculative=false \
    -D mapreduce.task.timeout=12000000 \
    -inputformat org.apache.hadoop.mapred.lib.NLineInputFormat \
    -input /ncdc_files.txt \
    -output /output/gsod \
    -mapper load_ncdc_map.sh \
    -file load_ncdc_map.sh
```
其中，load_ncdc_map.sh就是上面脚本的名称
之后需要等待一段时间

5.检查

通过命令
```
hdfs dfs -ls /NCDC_ALL
```
查看结果，若有一系列/NCDC_ALL/gsod_年份.gz的文件生成则表示执行生成

### 横向扩展
#### 数据流
MR作业(Job): 客户端需要执行的一个工作单元，包括输入数据，MR程序和配置信息

MR将作业分为若干个任务来执行，其中包括两类任务：map任务和reduce任务

Hadoop将MR的输入数据划分成等长的小数据块，称为输入分片(简称分片)。hadoop为每一个分片构建一个map任务。对于大多数作业来说，一个合理的分片大小趋向于HDFS的一个块的大小，因为如果分片大小大于HDFS数据块的大小，那么就需要将其中一块的数据想另一块传输，不符合“数据本地优化”，通俗的说就是在资源上进行计算(转移计算而不是转移资源)。

map任务在执行过程中会将结果记录在本地磁盘上，因为map的输出是中间结果，通过reduce处理后才会产生最终的结果，map的输出在结束后就会被删除，所以没有必要保存到HDFS中。

reduce的输出通常都存储在HDFS中以实现可靠存储，对于reduce输出的每个HDFS块，第一个副本是存储在本地节点上的，其余副本出于可靠性考虑会存储在其他机架的节点中

reduce的分区是单独指定的，对于map任务输出的每一个key，都只会在一个reduce任务中，但是一个reduce可以对应多个key
