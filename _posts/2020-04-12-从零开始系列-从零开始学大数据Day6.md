---
layout:     post
title:      从零开始学大数据
subtitle:   Day6
date:       2020-04-12
author:     KaithyXu
header-img: img/lake1.jpg
catalog: true
tags:
    - Big Data
---

## MapReduce -- 《Hadoop权威指南》

### Hadoop 文件数据读取

1.从Hadoop URL中读取数据

```
static {
    //java.net.URL,限制：每一个Java虚拟机只能调用一次 setURLStreamHandlerFactory
    URL.setURLStreamHandlerFactory(new FsUrlStreamHandlerFactory());
}
public static void main(String[] args) {
    System.out.println("the args is "+args[0]);
    InputStream in = null;
    try {
        in = new URL(args[0]).openStream();
        IOUtils.copyBytes(in, System.out, 4096, false);
    } catch (MalformedURLException e) {
        e.printStackTrace();
    } catch (IOException e) {
        e.printStackTrace();
    }finally {
        IOUtils.closeStream(in);
    }
}
```

2.通过FileSystem API读取数据
Hadoop 文件系统中通过Hadoop Path对象来代表文件，可以将路径视作一个Hadoop文件系统URI，如hdfs://localhost/ncdc/sample.txt

```
public static void main(String[] args) throws IOException {
    String uri = args[0];
    System.out.println("the uri is "+uri);

    Configuration conf = new Configuration();
    //org.apache.hadoop.fs.FileSystem
    FileSystem fs = FileSystem.get(URI.create(uri), conf);
    InputStream in = null;
    try{
        in = fs.open(new Path(uri));
        IOUtils.copyBytes(in, System.out, 4096, false);
    }finally {
        IOUtils.closeStream(in);
    }
}

```

实际上，FileSystem.open()返回的是FSDataInputStream对象，继承了java.io.DataInputStream ，支持随机访问，因此可以从流的任意位置读取数据
![](https://github.com/KaithyRookie/KaithyRookie/blob/master/imgs/FSDataInputStream.jpg)

Seekable接口提供了一个查询 当前位置相对于文件起始位置偏移量的方法getPos()， seek()可以移动到文件中的任意一个绝对位置
![](https://github.com/KaithyRookie/KaithyRookie/blob/master/imgs/Seekable.jpg)

注: seek()是一个相对高开销的操作，建议使用流数据来构建应用的访问模式，而不是执行大量的seek()方法

PositionedReadable接口支持从一个指定偏移量处读取文件的一部分

![](https://github.com/KaithyRookie/KaithyRookie/blob/master/imgs/PositionedReadable.jpg)

文件读取流程

![](https://github.com/KaithyRookie/KaithyRookie/blob/master/imgs/HadoopRead.jpg)

1. Clinet 通过调用FileSystem.open()方法打开希望读取的文件，对于HDFS而言，这个对象是 DistrubutedFileSystem的一个实例
2. DistrubutedFileSystem通过RPC（远程过程调用），请求namenode，获取所要读取的文件的位置，namenode返回春游该块副本的datanode地址，若Client本身是一个datanode（例如MR中的mapper任务），那么客户端将会从保存有相应数据块副本的本地datanode读取数据。DistrubutedFileSystem类会返回一个FSDataInputStream（封装了DFSInputStream对象，管理datanode和namenode的I/O）对象给Client以便读取数据
3. 客户端对输入流调用read() 方法，存储着datanode地址的DFSInputStream随即连接距离最近的文件中第一块所在的datanode
4. 对数据流反复调用read()，可将数据从datanode读取到Client中，直到块的末尾，DFSInputStream关闭了与该datanode的连接，然后寻找文件下一块的最佳datanode

问：DFSInputStream如何判断哪个节点最近？
答：Hadoop 将网络看作一颗树，两个节点之间的距离是它们到最近共同祖先的距离总和。可分为四种场景
* 同一节点上的进程 distance =0
* 同一机架上的不同节点 distance = 2
* 同一数据中心中不同机架上的节点 distance = 4
* 不同数据中心中的节点 distance = 6

### 文件数据写入：

```
public static void main(String[] args) throws IOException {


    String localSrc = args[0];
    String dest = args[1];

    InputStream ins = new BufferedInputStream(new FileInputStream(localSrc));

    Configuration conf = new Configuration();
    FileSystem fs = FileSystem.get(URI.create(dest),conf);


    OutputStream out = fs.create(new Path(dest), new Progressable() {
        @Override
        public void progress() {
            System.out.print(".");
        }
    });
    IOUtils.copyBytes(ins, out, 4096, true);
}

```

重载的Progressable方法用于传递回调接口，可以将每一次数据写入datanode的进度通知给应用。

HDFS只允许对一个已打开的文件顺序写入，或在现有文件末尾追加数据，不支持在除文件末尾之外的其他位置进行写入

文件写入流程：

![](https://github.com/KaithyRookie/KaithyRookie/blob/master/imgs/HadoopWrite.jpg)

1. Client通过DistrubutedFileSystem对象调用create()来创建文件。
2. DistrubutedFileSystem对namenode创建一个RPC调用，在文件系统的命名空间中新建一个文件
3. namenode执行检查以确保创建的文件不存在切Client有创建文件的权限 ，若检查通过则会为新文件创建一条记录
4. DistrubutedFileSystem向Client返回一个FSDataOutputStream对象，由此客户端开始写入数据, FSDataOutputStream 封装了DFSoutPutStream对象，该对象负责datanode和namenode之间的通信
5. 当Client写入数据时，DFSoutPutStream将数据分成一个个数据包，存到“数据队列”，由DataStreamer处理数据队列，DataStreamer负责挑选出适合存储数据副本的一组datanode，这些datanode组成一个管线，DataStreamer将数据包流式的传给第一个datanode，datanode存储数据包后，将数据包发送给第二个，依次操作
6. DFSoutPutStream维护一个内部数据包队列(确认队列)等待datanode 的ack 消息，收到管道中的所有datanode确认消息后，才会将数据包从确认队列中删除
7. Client完成数据写入后，对数据流调用close，该操作将剩余的所有数据包写入datanode管线，并联系到namenode告知其文件写入完成之前，等待确认

问：写入操作何时认为写入成功？
答：在一块被写入期间，只要有dfs.namenode.replication.min的副本数（默认为1）写入成功，则写操作成功，并且这个块可以在集群中一步复制，直到达到目标副本数(dfs.replication, 默认为3)

Hadoop的默认布局策略：在运行Client的节点上放第一个副本，第二个副本与第一个 不同机架的节点上，第三个副本与第二个副本在同以】机架不同节点上

### 其他
#### 文件元数据 FileStatus

FileStatus类封装了文件系统中文件和目录的元数据，包括文件长度，块大小，副本，修复时间，所有者以及权限信息

#### 一致性模型

写入文件的内容不能保证立即可见，当前正在写入的块对其他reader不可见

HDFS提供了强行将所有缓存刷新到datanode 的手段，即通过调用FSDataOutputStream的hflush()方法，当 hflush方法返回成功后，对所有新的reader而言，HDFS可以保证文件中到目前为止写入的数据均到达所有datanode的写入管道并且对所有新的reader可见
但是hflush仅仅是将数据写进datanode 的内存中，还未完全落盘。
注：close()方法隐含调用了hflush()

而另一个方法hsync() 则是当数据都在各datanode 中落盘后才会返回，开销比hflush更大

#### 文件数据复制

Hadoop自带distcp 程序， 可并行从Hadoop文件系统中复制大量数据，也可将大量数据复制到Hadoop中

操作:

```
hadoop distcp srcFile destFile

Hadoop distcp srcDir destDir

-update 用于指示只更新不一样的内容；
-overwrite用于指示覆盖
-delete 运行distcp在destDir中 删除srcDir中没有的内容
-P 保留文件状态属性

hadoop distcp -update srcDir destDir

```