---
layout:     post
title:      线性回归
subtitle:   线性回归
date:       2020-01-09
author:     KaithyXu
header-img: img/linearRegression.jpg
catalog: true
tags:
    - Machine Learning
---

## 线性回归


### 原理

在统计学中，线性回归（英語：linear regression）是利用称为线性回归方程的最小平方函數对一个或多个自变量和因变量之间关系进行建模的一种回归分析。这种函数是一个或多个称为回归系数的模型参数的线性组合。只有一个自变量的情况称为简单回归，大于一个自变量情况的叫做多元回归（multivariable linear regression）。

有数据集![](https://latex.codecogs.com/gif.latex?%5C%7B%28x_1%2Cy_1%29%2C%28x_2%2Cy_2%29%2C...%2C%28x_n%2Cy_n%29%5C%7D),其中,![](https://latex.codecogs.com/gif.latex?x_i%20%3D%20%28x_%7Bi1%7D%3Bx_%7Bi2%7D%3Bx_%7Bi3%7D%3B...%3Bx_%7Bid%7D%29%2Cy_i%5Cin%20R) 
n表示变量的数量，d表示每个变量的维度。  
可以用以下函数来描述y和x之间的关系：
其一般表达式如下：
![](https://latex.codecogs.com/gif.latex?%5Cbegin%7Balign*%7D%20f%28x%29%20%26%3D%20%5Ctheta_0%20&plus;%20%5Ctheta_1x_1%20&plus;%20%5Ctheta_2x_2%20&plus;%20...%20&plus;%20%5Ctheta_dx_d%20%5C%5C%20%26%3D%20%5Csum_%7Bi%3D0%7D%5E%7Bd%7D%5Ctheta_ix_i%20%5C%5C%20%5Cend%7Balign*%7D)

用向量![](https://latex.codecogs.com/gif.latex?%5Ctheta)表示所有系数，训练的目的在于确定系数![](https://latex.codecogs.com/gif.latex?%5Ctheta)以使得f(x)尽可能的接近y的值。
均方误差是回归中常用的性能度量，即:
![](https://latex.codecogs.com/gif.latex?J%28%5Ctheta%29%3D%5Cfrac%7B1%7D%7B2%7D%5Csum_%7Bj%3D1%7D%5E%7Bn%7D%28h_%7B%5Ctheta%7D%28x%5E%7B%28i%29%7D%29-y%5E%7B%28i%29%7D%29%5E2)

### 极大似然估计
为何使用均方误差作为性能的度量，这可以用极大似然估计，从概率的角度来诠释。
可以将目标值与变量写成如下等式:
![](https://latex.codecogs.com/gif.latex?y%5E%7B%28i%29%7D%20%3D%20%5Ctheta%5ET%20x%5E%7B%28i%29%7D&plus;%5Cepsilon%5E%7B%28i%29%7D)

![](https://latex.codecogs.com/gif.latex?%5Cepsilon)表示我们未观测到的变量的印象，即随机噪音。我们假定![](https://latex.codecogs.com/gif.latex?%5Cepsilon)是独立同分布，服从高斯分布。（根据中心极限定理）
![](https://latex.codecogs.com/gif.latex?p%28%5Cepsilon%5E%7B%28i%29%7D%29%20%3D%20%5Cfrac%7B1%7D%7B%5Csqrt%7B2%5Cpi%7D%5Csigma%7Dexp%5Cleft%28-%5Cfrac%7B%28%5Cepsilon%5E%7B%28i%29%7D%29%5E2%7D%7B2%5Csigma%5E2%7D%5Cright%29)
因此：
![](https://latex.codecogs.com/png.latex?p%28y%5E%7B%28i%29%7D%7Cx%5E%7B%28i%29%7D%3B%5Ctheta%29%20%3D%20%5Cfrac%7B1%7D%7B%5Csqrt%7B2%5Cpi%7D%5Csigma%7Dexp%5Cleft%28-%5Cfrac%7B%28y%5E%7B%28i%29%7D-%5Ctheta%5ET%20x%5E%7B%28i%29%7D%29%5E2%7D%7B2%5Csigma%5E2%7D%5Cright%29)
我们建立极大似然函数，即描述数据遵从当前样本分布的概率分布函数。由于样本的数据集独立同分布，因此可以写成:
![](https://latex.codecogs.com/png.latex?L%28%5Ctheta%29%20%3D%20p%28%5Cvec%20y%20%7C%20X%3B%5Ctheta%29%20%3D%20%5Cprod%5En_%7Bi%3D1%7D%5Cfrac%7B1%7D%7B%5Csqrt%7B2%5Cpi%7D%5Csigma%7Dexp%5Cleft%28-%5Cfrac%7B%28y%5E%7B%28i%29%7D-%5Ctheta%5ET%20x%5E%7B%28i%29%7D%29%5E2%7D%7B2%5Csigma%5E2%7D%5Cright%29)

选择![](https://latex.codecogs.com/gif.latex?%5Ctheta)，使得似然函数最大化，这就是极大似然估计的思想。

为方便计算，通常对对数似然函数求最大值:
![](https://latex.codecogs.com/png.latex?%5Cbegin%7Balign*%7D%20l%28%5Ctheta%29%20%26%3D%20log%20L%28%5Ctheta%29%20%3D%20log%20%5Cprod%5En_%7Bi%3D1%7D%5Cfrac%7B1%7D%7B%5Csqrt%7B2%5Cpi%7D%5Csigma%7Dexp%5Cleft%28-%5Cfrac%7B%28y%5E%7B%28i%29%7D-%5Ctheta%5ET%20x%5E%7B%28i%29%7D%29%5E2%7D%20%7B2%5Csigma%5E2%7D%5Cright%29%20%5C%5C%20%26%20%3D%20%5Csum%5En_%7Bi%3D1%7Dlog%5Cfrac%7B1%7D%7B%5Csqrt%7B2%5Cpi%7D%5Csigma%7Dexp%5Cleft%28-%5Cfrac%7B%28y%5E%7B%28i%29%7D-%5Ctheta%5ET%20x%5E%7B%28i%29%7D%29%5E2%7D%7B2%5Csigma%5E2%7D%5Cright%29%20%5C%5C%20%26%20%3D%20nlog%5Cfrac%7B1%7D%7B%7B%5Csqrt%7B2%5Cpi%7D%5Csigma%7D%7D%20-%20%5Cfrac%7B1%7D%7B%5Csigma%5E2%7D%20%5Ccdot%20%5Cfrac%7B1%7D%7B2%7D%5Csum%5En_%7Bi%3D1%7D%28%28y%5E%7B%28i%29%7D-%5Ctheta%5ET%20x%5E%7B%28i%29%7D%29%5E2%20%5Cend%7Balign*%7D)

显然，最大化![](https://latex.codecogs.com/png.latex?l%28%5Ctheta%29)即最小化 ![](https://latex.codecogs.com/png.latex?%5Cfrac%7B1%7D%7B2%7D%5Csum%5En_%7Bi%3D1%7D%28%28y%5E%7B%28i%29%7D-%5Ctheta%5ET%20x%5E%7B%28i%29%7D%29%5E2)

这一结果即均方误差，因此用这个值作为代价函数来优化模型在统计学的角度是合理的。

中心极限定理:概率论中的一组定理，指在适当的条件下，大量相互独立的随机变量的均值经过适当的标准化后依分布收敛于正太分布
高斯分布:又名正态分布，遵从正太分布的我随机变量的概率为去邻近未知参数的值的概率大，远的值的概率小
![](img/normal_distribution.png)


### 线性回归损失函数、代价函数、目标函数
* 损失函数(Loss Function)：度量单样本预测的错误程度，损失函数值越小，模型就越好。
* 代价函数(Cost Function)：度量全部样本集的平均误差。
* 目标函数(Object Function)：代价函数和正则化函数，最终要优化的函数。
* 
常用的损失函数包括：0-1损失函数、平方损失函数、绝对损失函数、对数损失函数等；常用的代价函数包括均方误差、均方根误差、平均绝对误差等。

### 线性回归的优化方法
1. 随机梯度下降算法
设定初始参数![](https://latex.codecogs.com/gif.latex?%5Ctheta)，使得![](https://latex.codecogs.com/png.latex?J%28%5Ctheta%29)最小化: ![](https://latex.codecogs.com/png.latex?%5Ctheta_j%3A%3D%5Ctheta_j-%5Calpha%5Cfrac%7B%5Cpartial%7BJ%28%5Ctheta%29%7D%7D%7B%5Cpartial%5Ctheta%7D)

![](https://latex.codecogs.com/png.latex?%5Cbegin%7Balign*%7D%20%5Cfrac%7B%5Cpartial%7BJ%28%5Ctheta%29%7D%7D%7B%5Cpartial%5Ctheta%7D%20%26%3D%20%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial%5Ctheta_j%7D%5Cfrac%7B1%7D%7B2%7D%5Csum_%7Bi%3D1%7D%5E%7Bn%7D%28f_%5Ctheta%28x%29%5E%7B%28i%29%7D-y%5E%7B%28i%29%7D%29%5E2%20%5C%5C%20%26%3D%202*%5Cfrac%7B1%7D%7B2%7D%5Csum_%7Bi%3D1%7D%5E%7Bn%7D%28f_%5Ctheta%28x%29%5E%7B%28i%29%7D-y%5E%7B%28i%29%7D%29*%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial%5Ctheta_j%7D%28f_%5Ctheta%28x%29%5E%7B%28i%29%7D-y%5E%7B%28i%29%7D%29%20%5C%5C%20%26%3D%20%5Csum_%7Bi%3D1%7D%5E%7Bn%7D%28f_%5Ctheta%28x%29%5E%7B%28i%29%7D-y%5E%7B%28i%29%7D%29*%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial%5Ctheta_j%7D%28%5Csum_%7Bj%3D0%7D%5E%7Bd%7D%5Ctheta_jx_j%5E%7B%28i%29%7D-y%5E%7B%28i%29%7D%29%29%5C%5C%20%26%3D%20%5Csum_%7Bi%3D1%7D%5E%7Bn%7D%28f_%5Ctheta%28x%29%5E%7B%28i%29%7D-y%5E%7B%28i%29%7D%29x_j%5E%7B%28i%29%7D%20%5C%5C%20%5Cend%7Balign*%7D)

即:
![](https://latex.codecogs.com/png.latex?%5Ctheta_j%20%3D%20%5Ctheta_j%20&plus;%20%5Calpha%5Csum_%7Bi%3D1%7D%5E%7Bn%7D%28y%5E%7B%28i%29%7D-f_%5Ctheta%28x%29%5E%7B%28i%29%7D%29x_j%5E%7B%28i%29%7D)
其中，下标j表示第j个参数，上标i表示第i个数据点。

将所有的参数以向量形式表示，可得：
![](https://latex.codecogs.com/png.latex?%5Ctheta%20%3D%20%5Ctheta%20&plus;%20%5Calpha%5Csum_%7Bi%3D1%7D%5E%7Bn%7D%28y%5E%7B%28i%29%7D-f_%5Ctheta%28x%29%5E%7B%28i%29%7D%29x%5E%7B%28i%29%7D)

在随机梯度下降算法中，每次随机选择一个样本根据函数计算最新的![](https://latex.codecogs.com/gif.latex?%5Ctheta)，即:
![](https://latex.codecogs.com/png.latex?%5Ctheta%20%3D%20%5Ctheta%20&plus;%20%5Calpha%28y%5E%7B%28i%29%7D-f_%5Ctheta%28x%29%5E%7B%28i%29%7D%29x%5E%7B%28i%29%7D)
并计算![](https://latex.codecogs.com/png.latex?f%28x%29%5E%7B%28i%29%7D) 与![](https://latex.codecogs.com/png.latex?y%5E%7B%28i%29%7D)的误差的平方

优点: 当数据点足够多的时候，允许效率更高
缺点: 因为每次只针对一个样本更新参数，未必找到最快路径达到最优值，甚至有时候会出现参数在最小值附近徘徊而不是立即收敛

当数据量很大的时候，随机梯度下降法经常优于批梯度下降法

梯度下降法的缺陷在于当函数为非凸函数时，有可能找到的并非全局最优值，而是局部最优值

2. 最小二乘法矩阵求解
令:
![](https://latex.codecogs.com/png.latex?X%20%3D%20%5Cleft%5B%20%5Cbegin%7Barray%7D%20%7Bcccc%7D%20%28x%5E%7B%281%29%7D%29%5ET%5C%5C%20%28x%5E%7B%282%29%7D%29%5ET%5C%5C%20%5Cldots%20%5C%5C%20%28x%5E%7B%28n%29%7D%29%5ET%20%5Cend%7Barray%7D%20%5Cright%5D)

其中:
![](https://latex.codecogs.com/png.latex?x%5E%7B%28i%29%7D%20%3D%20%5Cleft%5B%20%5Cbegin%7Barray%7D%20%7Bcccc%7D%20x_1%5E%7B%28i%29%7D%5C%5C%20x_2%5E%7B%28i%29%7D%5C%5C%20%5Cldots%20%5C%5C%20x_d%5E%7B%28i%29%7D%20%5Cend%7Barray%7D%20%5Cright%5D)

由于:
![](https://latex.codecogs.com/png.latex?Y%20%3D%20%5Cleft%5B%20%5Cbegin%7Barray%7D%20%7Bcccc%7D%20y%5E%7B%281%29%7D%5C%5C%20y%5E%7B%282%29%7D%5C%5C%20%5Cldots%20%5C%5C%20y%5E%7B%28n%29%7D%20%5Cend%7Barray%7D%20%5Cright%5D)

![](https://latex.codecogs.com/png.latex?h_%5Ctheta%28x%29)可以写作![](https://latex.codecogs.com/png.latex?h_%5Ctheta%28x%29%3DX%5Ctheta)

对于向量来说，有![](https://latex.codecogs.com/png.latex?z%5ETz%20%3D%20%5Csum_i%20z_i%5E2)
因此可以把损失函数写作
![](https://latex.codecogs.com/png.latex?J%28%5Ctheta%29%3D%5Cfrac%7B1%7D%7B2%7D%28X%5Ctheta-Y%29%5ET%28X%5Ctheta-Y%29)

为最小化![](https://latex.codecogs.com/png.latex?J%28%5Ctheta%29),对![](https://latex.codecogs.com/png.latex?%5Ctheta)求导可得：
![](https://latex.codecogs.com/png.latex?%5Cbegin%7Balign*%7D%20%5Cfrac%7B%5Cpartial%7BJ%28%5Ctheta%29%7D%7D%7B%5Cpartial%5Ctheta%7D%20%26%3D%20%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial%5Ctheta%7D%20%5Cfrac%7B1%7D%7B2%7D%28X%5Ctheta-Y%29%5ET%28X%5Ctheta-Y%29%20%5C%5C%20%26%3D%20%5Cfrac%7B1%7D%7B2%7D%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial%5Ctheta%7D%20%28%5Ctheta%5ETX%5ETX%5Ctheta%20-%20Y%5ETX%5Ctheta-%5Ctheta%5ET%20X%5ETY%20-%20Y%5ETY%29%20%5C%5C%20%5Cend%7Balign*%7D)

中间两项互为转置，由于求得的值是个标量，矩阵与转置相同，因此可以写成
![](https://latex.codecogs.com/png.latex?%5Cbegin%7Balign*%7D%20%5Cfrac%7B%5Cpartial%7BJ%28%5Ctheta%29%7D%7D%7B%5Cpartial%5Ctheta%7D%20%26%3D%20%5Cfrac%7B1%7D%7B2%7D%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial%5Ctheta%7D%20%28%5Ctheta%5ETX%5ETX%5Ctheta%20-%202%5Ctheta%5ET%20X%5ETY%20-%20Y%5ETY%29%20%5C%5C%20%5Cend%7Balign*%7D)
令偏导数等于零，由于最后一项和 𝜃 无关，偏导数为0
因此:
![](https://latex.codecogs.com/png.latex?%5Cfrac%7B%5Cpartial%7BJ%28%5Ctheta%29%7D%7D%7B%5Cpartial%5Ctheta%7D%20%3D%20%5Cfrac%7B1%7D%7B2%7D%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial%5Ctheta%7D%20%5Ctheta%5ETX%5ETX%5Ctheta%20-%20%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial%5Ctheta%7D%20%5Ctheta%5ET%20X%5ETY)
利用矩阵求导性质,![](https://latex.codecogs.com/png.latex?%5Cfrac%7B%5Cpartial%20%5Cvec%20x%5ET%5Calpha%7D%7B%5Cpartial%20%5Cvec%20x%7D%20%3D%5Calpha)和
![](https://latex.codecogs.com/png.latex?%5Cfrac%7B%5Cpartial%20A%5ETB%7D%7B%5Cpartial%20%5Cvec%20x%7D%20%3D%20%5Cfrac%7B%5Cpartial%20A%5ET%7D%7B%5Cpartial%20%5Cvec%20x%7DB%20&plus;%20%5Cfrac%7B%5Cpartial%20B%5ET%7D%7B%5Cpartial%20%5Cvec%20x%7DA)

![](https://latex.codecogs.com/png.latex?%5Cbegin%7Balign*%7D%20%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial%5Ctheta%7D%20%5Ctheta%5ETX%5ETX%5Ctheta%20%26%3D%20%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial%5Ctheta%7D%7B%28X%5Ctheta%29%5ETX%5Ctheta%7D%5C%5C%20%26%3D%20%5Cfrac%7B%5Cpartial%20%28X%5Ctheta%29%5ET%7D%7B%5Cpartial%5Ctheta%7DX%5Ctheta%20&plus;%20%5Cfrac%7B%5Cpartial%20%28X%5Ctheta%29%5ET%7D%7B%5Cpartial%5Ctheta%7DX%5Ctheta%20%5C%5C%20%26%3D%202X%5ETX%5Ctheta%20%5Cend%7Balign*%7D)

![](https://latex.codecogs.com/png.latex?%5Cfrac%7B%5Cpartial%7BJ%28%5Ctheta%29%7D%7D%7B%5Cpartial%5Ctheta%7D%20%3D%20X%5ETX%5Ctheta%20-%20X%5ETY)
令导数等于零，![](https://latex.codecogs.com/png.latex?%5Ctheta%20%3D%20%28X%5ETX%29%5E%7B%28-1%29%7DX%5ETY)

### 代码实现

```python
#生成数据
import numpy as np
#生成随机数
np.random.seed(1234)
x = np.random.rand(500,3)
#构建映射关系，模拟真实的数据待预测值,映射关系为y = 4.2 + 5.7*x1 + 10.8*x2，可自行设置值进行尝试
y = x.dot(np.array([4.2,5.7,10.8]))
```

1. 最小二乘法的矩阵求解：

```python
class LR_LS():
    def __init__(self):
        self.w = None      
    def fit(self, X, y):
        # 最小二乘法矩阵求解
        #============================= show me your code =======================
        # X进行转置为列向量
        XT = X.transpose()
        
        # 计算向量X与X转置矩阵乘积的逆
        XXT_Inv = np.linalg.inv(XT.dot(X))
        
        # 计算X的转置与y的乘积
        XTy = XT.dot(y)
        
        self.w = XXT_Inv.dot(XTy)
        #============================= show me your code =======================
    def predict(self, X):
        # 用已经拟合的参数值预测新自变量
        #============================= show me your code =======================
        
        y_pred = X.dot(self.w)
        #============================= show me your code =======================
        return y_pred

if __name__ == "__main__":
    lr_ls = LR_LS()
    lr_ls.fit(x,y)
    print("估计的参数值：%s" %(lr_ls.w))
    x_test = np.array([2,4,5]).reshape(1,-1)
    print("预测值为: %s" %(lr_ls.predict(x_test)))


```

2. 随机梯度下降算法实现

```python
class LR_GD():
    def __init__(self):
        self.w = None     
    def fit(self,X,y,alpha=0.02,loss = 1e-10): # 设定步长为0.002,判断是否收敛的条件为1e-10
        y = y.reshape(-1,1) #重塑y值的维度以便矩阵运算
        [m,d] = np.shape(X) #自变量的维度
        self.w = np.zeros((d)) #将参数的初始值定为0
        tol = 1e5
        #============================= show me your code =======================
        while tol > loss:
            # here
            #获取随机样本
            rand = np.random.randint(0,m,1)
            
            #计算随机样本的预测结果
            rp = X[rand].dot(self.w)
            
            #根据算法更新𝜃
            self.w = self.w + alpha * (y[rand] - rp)*X[rand]
            
            #将𝜃转置为列向量
            self.w = np.sum(self.w,axis=0)
            
            #根据新的估计参数值计算误差平方和
            tol = (rp - y[rand])**2
        #============================= show me your code =======================
    def predict(self, X):
        # 用已经拟合的参数值预测新自变量
        y_pred = X.dot(self.w)
        return y_pred  

if __name__ == "__main__":
    lr_gd = LR_GD()
    lr_gd.fit(x,y)
    print("估计的参数值为：%s" %(lr_gd.w))
    x_test = np.array([2,4,5]).reshape(1,-1)
    print("预测值为：%s" %(lr_gd.predict(x_test)))


```

### 参考文献
👉[维基百科-中心极限定理](https://zh.wikipedia.org/wiki/%E4%B8%AD%E5%BF%83%E6%9E%81%E9%99%90%E5%AE%9A%E7%90%86)

👉[中心极限定理通俗介绍](https://zhuanlan.zhihu.com/p/25241653)

👉[极大似然估计](https://www.jianshu.com/p/e0eb4f4ccf3e)

👉[深入理解机器学习中的：目标函数，损失函数和代价函数](https://blog.csdn.net/qq_28448117/article/details/79199835)