---
layout:     post
title:      从零开始学大数据
subtitle:   Day5
date:       2020-03-29
author:     KaithyXu
header-img: img/cat.jpg
catalog: true
tags:
    - Big Data
---

## MapReduce -- 《Hadoop权威指南》

### Hadoop 分布式文件系统

管理网络中跨多台计算机存储的文件系统统称为分布式文件系统

HDFS的特性：
* 超大文件
* 流式数据访问，一次写入，多次读取
* 商用硬件
* 较高时间延迟的数据访问，以提供时间延迟为代价对高数据吞吐量进行优化
* 大量的小文件，由于namenode将文件系统的元数据存储在内存中，因此HDFS所能存储的文件总数受限于namenode的内存容量
* 单用户写入
* 追加的写入模式，不支持在文件的任意位置进行修改

### HDFS相关概念

#### 数据块

每个磁盘都有默认的数据块大小，是磁盘进行数据读写的最小单位，构建于单个磁盘之上的文件系统通过磁盘块来管理该文件系统中的块。

HDFS中也有块(block)的概念，默认为128M，在HDFS上的文件被划分为块大小的多个分块(chunk)，作为独立的存储单元。不同于文件系统的一个文件至少占据一个块空间(若文件比文件系统的块小，这个文件会占用一个块，块中多余的空间无法再被其余文件数据占用), HDFS中，一个块可以保存多个文件数据，按照文件的大小分配对应的存储空间(若文件大小为1M，那么该文件只占一个块空间的1M存储空间，还余127M空间可供使用)

HDFS中的块比磁盘块更大的原因是为了最小化磁盘的寻址开销，若干块足够大，那么从磁盘传输数据的时间会明显大于定位这个块开始位置所需的时间

使用数据块的优势：
1. 一个文件的大小可以大于集群中任意一个磁盘的容量，因为一个文件并不需要存储在同一个磁盘上
2. 极大的简化了存储子系统的设计
3. 数据块非常适用于数据备份进而提供数据容错能力和提高可用性

可以使用fsck命令显示块信息
如：以下命令可以列出文件系统中各个文件由哪些块构成
```
hdfs fsck / -files -blocks
```

#### namenode 和 datanode

HDFS以管理节点-工作节点的模式运作，即一个namenode和多个datanode

namenode职责：
1. 管理文件系统的命名空间，维护着文件系统树及整棵树内所有的文件和目录。这些信息以两个文件形式永久的保存在本地磁盘上: 命名空间镜像文件和编辑日志文件。
2. 记录着每个文件中各个块所在的数据节点信息，这些信息会在系统启动时根据数据节点信息重建

datanode职责:
1. 根据需要存储并检索数据块(受客户端或namenode调度)
2. 定期向namenode发送其锁存储的块的列表

#### 块缓存

通常datanode从磁盘读取块，但对于频繁访问的文件，其对应的块可能被显示的缓存在datanode的内存中，以堆外块缓存(off-heap block cache)的形式存在。

默认情况下，一个块仅缓存在一个datanode的内存中。用户或应用通过在缓存池中增加一个cache directive来告诉namenode需要缓存哪些文件以及缓存时长.(缓存池，用于管理缓存权限和资源使用的管理性分组)

缓存命令：
添加：
```
hdfs cacheadmin --addDirective -path path -pool pool-name [-force] [-replication replication] [-ttl time-to-live]
```

-path: 添加的路径
-pool: 加入的缓冲池名称
-force: 不检查缓存池的资源限制
-replication: 要使用的副本数, 默认为1
-ttl: 缓存保存时间,有效的单位包括[smhd]，可以按分钟，小时，天来指定，如 15m, 4h, 2d。若未指定ttl，则缓存不会过期

缓存池命令:
添加：
```
hdfs cacheadmin -addPool name [-owner owner] [-group group] [-mode mode] [-limit limit] [-maxTtl maxTtl]

```

-owner/group:分别指定该缓存池所属的用户与主，默认为当前用户
-mode：POSIX分割权限，默认0755
-limit：该缓存池中可以缓存的最大字节数，默认没有限制
-maxTtl：最大的生存周期，有效单位[smhd]

#### 联邦HDFS

Hadoop 2.x版本引入的联邦HDFS允许系统通过添加多个namenode实现扩展，每个namenode管理文件系统命名空间的一部分

在联邦环境下，每一个namenode维护一个命名空间卷（namespace volume），由命名空间的元数据和一个数据块池(block pool)组成，数据块池包含该命名空间下文件的所有数据块, 命名空间卷之间相互独立，不进行通信。

数据块池不再进行切分，因此集群中的每一个datanode都需要注册到每一个namenode，并且存储则来自多个数据块池中的数据块。

访问联邦集群时，客户端需要使用挂载数据表将文件路径映射到namenode。

联邦机制的引入是为了解决namenode内存的大小成为超大文件的超大集群的瓶颈而引入的，并不能解决namenode单点故障的问题。

#### HDFS的高可用性

通过联合使用多个文件系统中备份namenode的元数据和通过备用namenode创建监测点能防止数据丢失，但是无法解决单点故障问题。

Hadoop2中增加了对HDFS高可用性的支持。
两种高可用性共享存储方案：
1. NFS过滤器
2. 群体日子管理器（QJM），专用的HDFS实现，为提供一个高可用的编辑日志而设计，被推荐与大多数HDFS部署中。QJM以一组日志节点的形运行，每一次编辑必须写入多数日志节点(这种安排与Zookeeper的工作方式类似，但是QJM的实现并未依赖ZK，Hadoop在确定活动的namenode节点上依赖了ZK)

故障切换与规避
Hadoop中有一个称为故障转移控制器的新实体，管理着将活动namenode转移为备用namenode的转移过程。默认的使用Zookeeper来确保有且只有一个活动的namenode，其余namenode运行着一个轻量级的故障转移控制器，负责监视主namenode是否失效并在其失效时进行故障切换

#### Hadoop文件系统

Hadoop有一个抽象的文件系统概念，HDFS只是其中的一个实现，Java抽象类org.apache.hadoop.fs.FileSystem定义了Hadoop中一个文件系统的客户端接口


| 文件系统 | URI方案 | Java实现 | 描述 |
| --- | --- | --- | --- |
| Local | file | fs.LocalFIleSystem | 使用客户端校验和的能敌磁盘文件系统 |
| HDFS | hdfs | hdfs.DistributedFuleSystem | Hadoop的分布式文件系统 |
| WebHDFS | webhdfs | hdfs.web.WebHdfsFileSystem | 基于Http的文件系统，提供对HDFS的认证读写访问 |
| Secure WebHDFS | swebhdfs | hdfs.web.SWebHdfsFileSystem | WebHDFS的HTTPS版本 |
| HAR | har | fs.HarFileSystem | 一个构建在其他文件系统上用于文件存档的文件系统 |
| View | viewfs | viewfs.ViewFileSystem | 针对qitaHadoop文件系统的客户端挂载表 |
| FTP | ftp | fs.ftp.FTPFileSystem | 由FTP服务器支持的文件系统 |
| S3 | s3a | fs.s3a.S3AFileSystem | 有Amazon S3支持的文件系统 |
| Azure | wasb | fs.azure.NativeAzureFileSystem | 有Microsoft Azure支持的文件系统 |
| Swift | swift | fs.swift.snative.SwiftNativeFileSystem | 由OpenStack Swift支持的文件系统 |

Hadoop对文件系统提供了许多接口，一般使用URI方案来选去合适的文件系统实例来进行交互

